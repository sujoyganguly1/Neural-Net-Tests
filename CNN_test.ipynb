{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net\n",
    "In this notebook we will build a convolutional neural net (CNN) to classify handwritten numbers.\n",
    "To do this we will use the mnist data set.\n",
    "\n",
    "CNNs exploit spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. This is like the retina, but in a computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn.datasets\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import IPython.display\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data loader from mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset()\n",
    "# Create a dataset dictionary for convenience\n",
    "dataset = {\n",
    "    'train': {'X': X_train, 'y': y_train},\n",
    "    'valid': {'X': X_valid, 'y': y_valid}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 50000 examples for training. \n",
    "X_train is (50000, 1, 28, 28) and contains the 50000 images, each of which is gray scale (n_channels = 1) and 28x28 pixels.\n",
    "y_train is 50000, is a vector of numbers from 0-9 that are the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example digit with its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEKCAYAAAAy4ujqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3f6410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset['train']['X'][12][0], interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.title(\"Label: {}\".format(dataset['train']['y'][12]))\n",
    "plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural net\n",
    "Input layer has the shape of the data\n",
    "## Convolution Layer\n",
    "The basic 2D convolutional layer in Lasagne is Conv2DLayer. The layer works by convolution of the input layer with a linear filter adding a bias term and then applying a non-linear function.\n",
    "This generates a feature map\n",
    "We are using 32 learnable convolutional filters for this layer. Each layer has a 5x5 receptor field. \n",
    "## MaxPooling\n",
    "Max-pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = dataset['train']['X'][0].shape\n",
    "# input layer\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, input_shape[0], input_shape[1], input_shape[2]))\n",
    "# First convolutional layer\n",
    "l_conv1 = lasagne.layers.Conv2DLayer(l_in,\n",
    "    # Here, we set the number of filters and their size.\n",
    "    num_filters=32, \n",
    "    filter_size=(8, 8),\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    # Use He et. al.'s initialization\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "# Pool the layer with 2x2 Max pooling. Lasagne has the option to do mean pooling as well.\n",
    "l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1,pool_size=(2, 2))\n",
    "# The second convolution/pooling pair is the same as above.\n",
    "l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "    l_pool1, num_filters=32, filter_size=(8, 8),\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2, pool_size=(2, 2))\n",
    "# First hidden layer\n",
    "l_hidden1 = lasagne.layers.DenseLayer(\n",
    "    l_pool2, \n",
    "    # Why is the num_unit = 256 here? Seems arbitray.\n",
    "    num_units=196, \n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu'))\n",
    "# Add a dropout layer to pervent over classification\n",
    "l_hidden1_dropout = lasagne.layers.DropoutLayer(l_hidden1, p=0.2)\n",
    "# Output layer. num of units = 10 because y can take a value from 0-9\n",
    "l_output = lasagne.layers.DenseLayer(\n",
    "    l_hidden1_dropout,\n",
    "    # The number of units in the softmas output layer is the number of classes.\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_output = lasagne.layers.get_output(l_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training\n",
    "We define a loss function loss. As before, and is typical we are using the categorical crossentropy.\n",
    "For updates we will use Adaptive Learning Rate method for the Stocahstic Gradient Desent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_output = T.ivector('true_output')\n",
    "loss = T.mean(lasagne.objectives.categorical_crossentropy(net_output, true_output))\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_output)\n",
    "# Use ADADELTA for updates\n",
    "updates = lasagne.updates.adadelta(loss, all_params)\n",
    "train = theano.function([l_in.input_var, true_output], loss, updates=updates)\n",
    "\n",
    "# This is the function we'll use to compute the network's output given an input\n",
    "# (e.g., for computing accuracy).  Again, we don't want to apply dropout here\n",
    "# so we set the deterministic kwarg to True.\n",
    "get_output = theano.function([l_in.input_var],\n",
    "                             lasagne.layers.get_output(l_output, deterministic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We will chop the training data into mini-batches, and compute the validation accuracy every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Batch 2, Batch 3, Batch 4, Batch 5, Batch 6, Batch 7, Batch 8, Batch 9, Batch 10, Batch 11, Batch 12, Batch 13, Batch 14, Batch 15, Batch 16, Batch 17, Batch 18, Batch 19, Batch 20, Batch 21, Batch 22, Batch 23, Batch 24, Batch 25, Batch 26, Batch 27, Batch 28, Batch 29, Batch 30, Batch 31, Batch 32, Batch 33, Batch 34, Batch 35, Batch 36, Batch 37, Batch 38, Batch 39, Batch 40, Batch 41, Batch 42, Batch 43, Batch 44, Batch 45, Batch 46, Batch 47, Batch 48, Batch 49, Batch 50, Batch 51, Batch 52, Batch 53, Batch 54, Batch 55, Batch 56, Batch 57, Batch 58, Batch 59, Batch 60, Batch 61, Batch 62, Batch 63, Batch 64, Batch 65, Batch 66, Batch 67, Batch 68, Batch 69, Batch 70, Batch 71, Batch 72, Batch 73, Batch 74, Batch 75, Batch 76, Batch 77, Batch 78, Batch 79, Batch 80, Batch 81, Batch 82, Batch 83, Batch 84, Batch 85, Batch 86, Batch 87, Batch 88, Batch 89, Batch 90, Batch 91, Batch 92, Batch 93, Batch 94, Batch 95, Batch 96, Batch 97, Batch 98, Batch 99, Batch 100, Epoch 1 validation accuracy: 0.9528\n",
      "Batch 1, Batch 2, Batch 3, Batch 4, Batch 5, Batch 6, Batch 7, Batch 8, Batch 9, Batch 10, Batch 11, Batch 12, Batch 13, Batch 14, Batch 15, Batch 16, Batch 17, Batch 18, Batch 19, Batch 20, Batch 21, Batch 22, Batch 23, Batch 24, Batch 25, Batch 26, Batch 27, Batch 28, Batch 29, Batch 30, Batch 31, Batch 32, Batch 33, Batch 34, Batch 35, Batch 36, Batch 37, Batch 38, Batch 39, Batch 40, Batch 41, Batch 42, Batch 43, Batch 44, Batch 45, Batch 46, Batch 47, Batch 48, Batch 49, Batch 50, Batch 51, Batch 52, Batch 53, Batch 54, Batch 55, Batch 56, Batch 57, Batch 58, Batch 59, Batch 60, Batch 61, Batch 62, Batch 63, Batch 64, Batch 65, Batch 66, Batch 67, Batch 68, Batch 69, Batch 70, Batch 71, Batch 72, Batch 73, Batch 74, Batch 75, Batch 76, Batch 77, Batch 78, Batch 79, Batch 80, Batch 81, Batch 82, Batch 83, Batch 84, Batch 85, Batch 86, Batch 87, Batch 88, Batch 89, Batch 90, Batch 91, Batch 92, Batch 93, Batch 94, Batch 95, Batch 96, Batch 97, Batch 98, Batch 99, Batch 100, Epoch 2 validation accuracy: 0.9679\n",
      "Batch 1, Batch 2, Batch 3, Batch 4, Batch 5, Batch 6, Batch 7, Batch 8, Batch 9, Batch 10, Batch 11, Batch 12, Batch 13, Batch 14, Batch 15, Batch 16, Batch 17, Batch 18, Batch 19, Batch 20, Batch 21, Batch 22, Batch 23, Batch 24, Batch 25, Batch 26, Batch 27, Batch 28, Batch 29, Batch 30, Batch 31, Batch 32, Batch 33, Batch 34, Batch 35, Batch 36, Batch 37, Batch 38, Batch 39, Batch 40, Batch 41, Batch 42, Batch 43, Batch 44, Batch 45, Batch 46, Batch 47, Batch 48, Batch 49, Batch 50, Batch 51, Batch 52, Batch 53, Batch 54, Batch 55, Batch 56, Batch 57, Batch 58, Batch 59, Batch 60, Batch 61, Batch 62, Batch 63, Batch 64, Batch 65, Batch 66, Batch 67, Batch 68, Batch 69, Batch 70, Batch 71, Batch 72, Batch 73, Batch 74, Batch 75, Batch 76, Batch 77, Batch 78, Batch 79, Batch 80, Batch 81, Batch 82, Batch 83, Batch 84, Batch 85, Batch 86, Batch 87, Batch 88, Batch 89, Batch 90, Batch 91, Batch 92, Batch 93, Batch 94, Batch 95, Batch 96, Batch 97, Batch 98, Batch 99, Batch 100, Epoch 3 validation accuracy: 0.9797\n",
      "Batch 1, Batch 2, Batch 3, Batch 4, Batch 5, Batch 6, Batch 7, Batch 8, Batch 9, Batch 10, Batch 11, Batch 12, Batch 13, Batch 14, Batch 15, Batch 16, Batch 17, Batch 18, Batch 19, Batch 20, Batch 21, Batch 22, Batch 23, Batch 24, Batch 25, Batch 26, Batch 27, Batch 28, Batch 29, Batch 30, Batch 31, Batch 32, Batch 33, Batch 34, Batch 35, Batch 36, Batch 37, Batch 38, Batch 39, Batch 40, Batch 41, Batch 42, Batch 43, Batch 44, Batch 45, Batch 46, Batch 47, Batch 48, Batch 49, Batch 50, Batch 51, Batch 52, Batch 53, Batch 54, Batch 55, Batch 56, Batch 57, Batch 58, Batch 59, Batch 60, Batch 61, Batch 62, Batch 63, Batch 64, Batch 65, Batch 66, Batch 67, Batch 68, Batch 69, Batch 70, Batch 71, Batch 72, Batch 73, Batch 74, Batch 75, Batch 76, Batch 77, Batch 78, Batch 79, Batch 80, Batch 81, Batch 82, Batch 83, Batch 84, Batch 85, Batch 86, Batch 87, Batch 88, Batch 89, Batch 90, Batch 91, Batch 92, Batch 93, Batch 94, Batch 95, Batch 96, Batch 97, Batch 98, Batch 99, Batch 100, Epoch 4 validation accuracy: 0.9748\n",
      "Batch 1, Batch 2, Batch 3, Batch 4, Batch 5, Batch 6, Batch 7, Batch 8, Batch 9, Batch 10, Batch 11, Batch 12, Batch 13, Batch 14, Batch 15, Batch 16, Batch 17, Batch 18, Batch 19, Batch 20, Batch 21, Batch 22, Batch 23, Batch 24, Batch 25, Batch 26, Batch 27, Batch 28, Batch 29, Batch 30, Batch 31, Batch 32, Batch 33, Batch 34, Batch 35, Batch 36, Batch 37, Batch 38, Batch 39, Batch 40, Batch 41, Batch 42, Batch 43, Batch 44, Batch 45, Batch 46, Batch 47, Batch 48, Batch 49, Batch 50, Batch 51, Batch 52, Batch 53, Batch 54, Batch 55, Batch 56, Batch 57, Batch 58, Batch 59, Batch 60, Batch 61, Batch 62, Batch 63, Batch 64, Batch 65, Batch 66, Batch 67, Batch 68, Batch 69, Batch 70, Batch 71, Batch 72, Batch 73, Batch 74, Batch 75, Batch 76, Batch 77, Batch 78, Batch 79, Batch 80, Batch 81, Batch 82, Batch 83, Batch 84, Batch 85, Batch 86, Batch 87, Batch 88, Batch 89, Batch 90, Batch 91, Batch 92, Batch 93, Batch 94, Batch 95, Batch 96, Batch 97, Batch 98, Batch 99, Batch 100, Epoch 5 validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "N_EPOCHS = 5\n",
    "# Keep track of which batch we're training with\n",
    "batch_idx = 0\n",
    "# Keep track of which epoch we're on\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    # Extract the training data/label batch and update the parameters with it\n",
    "    train(dataset['train']['X'][batch_idx:batch_idx + BATCH_SIZE],\n",
    "          dataset['train']['y'][batch_idx:batch_idx + BATCH_SIZE])\n",
    "    batch_idx += BATCH_SIZE\n",
    "    print(\"Batch {}, \".format(batch_idx/BATCH_SIZE),end=\"\")\n",
    "    # Once we've trained on the entire training set...\n",
    "    if batch_idx >= dataset['train']['X'].shape[0]:\n",
    "        # Reset the batch index\n",
    "        batch_idx = 0\n",
    "        # Update the number of epochs trained\n",
    "        epoch += 1\n",
    "        # Compute the network's output on the validation data\n",
    "        val_output = get_output(dataset['valid']['X'])\n",
    "        # The predicted class is just the index of the largest probability in the output\n",
    "        val_predictions = np.argmax(val_output, axis=1)\n",
    "        # The accuracy is the average number of correct predictions\n",
    "        accuracy = np.mean(val_predictions == dataset['valid']['y'])\n",
    "        print(\"Epoch {} validation accuracy: {}\".format(epoch, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
